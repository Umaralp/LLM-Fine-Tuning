# -*- coding: utf-8 -*-
"""LLM Fine Tuning Workflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o8E8-2de4b7zKu-nanUB9pXDU5SCYCzU

# Fine-Tuning a Language Model with Custom Knowledge
"""

# Load Model
from transformers import pipeline

model_name = "Qwen/Qwen2.5-3B-Instruct"

ask_llm = pipeline(
    model= model_name,
    # device="cuda"
)

print(ask_llm("who is Junaid Umar?")[0]["generated_text"])

# Dataset: To teach the model who Junaid Umar is, we will need to design a custom dataset.
# Load Raw Dataset

!pip install datasets
from datasets import load_dataset

raw_data = load_dataset("json", data_files="/content/junaid-umar_wizard.json")
raw_data

# Preview Random Raw Dataset Sample
raw_data["train"][0]

# Tokenization

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    model_name
)

def preprocess(sample):
    sample = sample["prompt"] + "\n" + sample["completion"]

    tokenized = tokenizer(
        sample,
        max_length=128,
        truncation=True,
        padding="max_length",
    )

    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

data = raw_data.map(preprocess)

# Preview Tokenized Sample

print(data["train"][0])

# LoRA (Low Rank Adaptation)

!pip install peft
from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map = "cpu", # Changed from "cuda" to "cpu"
    # Removed torch_dtype = torch.float16 as it's typically for GPU
)

lora_config = LoraConfig(
    task_type = TaskType.CAUSAL_LM,
    target_modules = ["q_proj", "k_proj", "v_proj"]
)

model = get_peft_model(model, lora_config)

# Training / Fine Tuning

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    num_train_epochs=10,
    learning_rate=0.001,
    logging_steps=25,
    report_to="none", # Disable Weights & Biases logging
    per_device_train_batch_size=1, # Reduce batch size to save memory
    gradient_accumulation_steps=4, # Accumulate gradients over 4 steps
    optim='adamw_torch' # Use non-fused optimizer to avoid XLA conflict
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=data["train"]
)

trainer.train()

# Save Model on Disk

trainer.save_model("./my_qwen")
tokenizer.save_pretrained("./my_qwen")

#Test Fine-Tuned Model

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig

path = "/content/my_qwen"

config = PeftConfig.from_pretrained(path)
base = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)
model = PeftModel.from_pretrained(base, path)

tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)

inputs = tokenizer("Who is Junaid Umar?", return_tensors="pt").to(model.device)

output = model.generate(
    input_ids=inputs["input_ids"],
    attention_mask=inputs["attention_mask"]
)

print(tokenizer.decode(output[0]))